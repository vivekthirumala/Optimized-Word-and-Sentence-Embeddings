{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training Word and Sentence Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA_BPUwcJ30H"
      },
      "source": [
        "References:\r\n",
        "\r\n",
        "1) Author: Borchers, Oliver\r\n",
        "\r\n",
        "https://github.com/oborchers/Fast_Sentence_Embeddings\r\n",
        "\r\n",
        "2) Author: kawine\r\n",
        "\r\n",
        "https://github.com/kawine/usif\r\n",
        "\r\n",
        "3) Author: Radim Řehůřek\r\n",
        "\r\n",
        "https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/phrases.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6n5L4mW9PUw"
      },
      "source": [
        "# Training Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haFuT3l5o2Pe",
        "outputId": "6966e115-917f-44e0-c68e-335747fb0e15"
      },
      "source": [
        "# Loading Packages\r\n",
        "\r\n",
        "!pip install fse\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "\r\n",
        "import gensim\r\n",
        "import string\r\n",
        "import operator\r\n",
        "import pickle\r\n",
        "from collections import defaultdict\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from gensim.utils import any2unicode, any2utf8\r\n",
        "from gensim.models import word2vec, KeyedVectors\r\n",
        "from gensim.models.phrases import Phraser, Phrases\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "from fse import CSplitIndexedList\r\n",
        "from fse.models import Average, uSIF, sif\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "from tqdm import tqdm\r\n",
        "tqdm.pandas()\r\n",
        "pd.set_option('display.max_colwidth',-1)\r\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fse in /usr/local/lib/python3.6/dist-packages (0.1.15)\n",
            "Requirement already satisfied: wordfreq>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from fse) (2.3.2)\n",
            "Requirement already satisfied: smart-open>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from fse) (4.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from fse) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from fse) (1.4.1)\n",
            "Requirement already satisfied: gensim>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from fse) (3.8.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from fse) (5.4.8)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from fse) (1.19.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.6/dist-packages (from wordfreq>=2.2.1->fse) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from wordfreq>=2.2.1->fse) (2019.12.20)\n",
            "Requirement already satisfied: langcodes>=2 in /usr/local/lib/python3.6/dist-packages (from wordfreq>=2.2.1->fse) (2.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->fse) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.8.0->fse) (1.15.0)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.6/dist-packages (from langcodes>=2->wordfreq>=2.2.1->fse) (0.7.5)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsLSiIprqyR4"
      },
      "source": [
        "# Preprocess\r\n",
        "\r\n",
        "stop_words = stopwords.words('english')\r\n",
        "\r\n",
        "def prep(text):\r\n",
        "  data = str(text).lower()\r\n",
        "  data = re.sub(r'\\[.*?\\]', '', data)\r\n",
        "  data = data.translate(str.maketrans(\"\",\"\",string.punctuation.replace('_','').replace('@',''))).strip()\r\n",
        "  data = re.sub(r'\\d+','',data)\r\n",
        "  data = [i for i in data.split() if not any(x in i for x in ['@','http'])]\r\n",
        "  data = [i for i in data if not len(i)<2]\r\n",
        "  data = [lemmatizer.lemmatize(i) for i in data]\r\n",
        "  return data\r\n",
        "\r\n",
        "def prep_s2v(text):\r\n",
        "  return [x for x in bigram_phraser_model[prep(text)] if x not in stop_words]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnGbhTzqqy3X",
        "outputId": "624e0eb0-d2a8-44f2-bbac-1637957b3f29"
      },
      "source": [
        "# Loading data and preprocessing\r\n",
        "\r\n",
        "# tqdm._instances.clear()\r\n",
        "data = pd.read_csv('/content/drive/MyDrive/Application/Case Studies/Hotel Reviews - Topic Modelling/Hotel_Reviews.csv').sample(n=10000,random_state = 10)\r\n",
        "data['text'] = data['Negative_Review']\r\n",
        "data['processed_text'] = data['text'].progress_apply(prep)\r\n",
        "data = data[['text','processed_text']]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:02<00:00, 3713.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx-4iMF5uj6W",
        "outputId": "b337ab8d-b377-42b0-e448-1f678b90b3bc"
      },
      "source": [
        "# Training Phraser, Word2Vec, Sentence2Vec Models\r\n",
        "\r\n",
        "tqdm._instances.clear()\r\n",
        "bigram_phraser_model = Phrases(data['processed_text'], min_count=3, threshold=2.5)\r\n",
        "bigram_features = data['processed_text'].progress_apply(lambda x: [word for word in bigram_phraser_model[x] if word not in stop_words])\r\n",
        "w2v_model = word2vec.Word2Vec(bigram_features, size = 100, sg = 0, workers = 8, min_count = 3, window = 5, iter = 100)\r\n",
        "indexed_sentences = CSplitIndexedList(list(data['text']), custom_split=prep_s2v)\r\n",
        "s2v_model = uSIF(w2v_model, workers=2, lang_freq=\"en\")\r\n",
        "s2v_model.train(indexed_sentences)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:01<00:00, 9843.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9941, 87036)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8KGzSeKukB3"
      },
      "source": [
        "# Optimizing and saving Sentence2Vec Model\r\n",
        "\r\n",
        "optimized_model_s2v_usif = {}\r\n",
        "optimized_model_s2v_usif[\"word_index\"] = dict(zip(s2v_model.wv.index2word,range(len(s2v_model.wv.index2word))))\r\n",
        "optimized_model_s2v_usif[\"word_weights\"] = s2v_model.word_weights\r\n",
        "optimized_model_s2v_usif[\"word_vectors\"] = s2v_model.wv.vectors\r\n",
        "optimized_model_s2v_usif[\"svd_residuals\"] = s2v_model.svd_res\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/Application/Case Studies/W2V S2V Optimization/optimized_model_s2v_usif.pkl','wb') as f:\r\n",
        "  pickle.dump(optimized_model_s2v_usif, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLNRw6lxs5q6"
      },
      "source": [
        "# Optimizing and saving Phraser Model\r\n",
        "\r\n",
        "phraser_dict = {}\r\n",
        "phraser_dict[\"vocab_length\"] = len(bigram_phraser_model.vocab)\r\n",
        "bigram_phraser_model.vocab = defaultdict(int, dict([(key,value) for key,value in bigram_phraser_model.vocab.items() \r\n",
        "                                                      if any2unicode(key) in optimized_model_s2v_usif[\"word_index\"]]))\r\n",
        "phraser_dict[\"phraser_model\"] = bigram_phraser_model\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/Application/Case Studies/W2V S2V Optimization/optimized_model_bigram_phraser.pkl','wb') as f:\r\n",
        "  pickle.dump(phraser_dict, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7CcV3Y29Usm"
      },
      "source": [
        "# Using Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h7ezsmp3EiI"
      },
      "source": [
        "import pickle\r\n",
        "import re\r\n",
        "import string\r\n",
        "from numpy import float32 as REAL, sum as np_sum, multiply as np_mult\r\n",
        "from gensim.utils import any2utf8, any2unicode\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_krv817Z890U"
      },
      "source": [
        "# Preprocess input sentence and convert to phrases\r\n",
        "\r\n",
        "common_terms = frozenset()\r\n",
        "def prep(text):\r\n",
        "  data = str(text).lower()\r\n",
        "  data = re.sub(r'\\[.*?\\]', '', data)\r\n",
        "  data = data.translate(str.maketrans(\"\",\"\",string.punctuation.replace('_','').replace('@',''))).strip()\r\n",
        "  data = re.sub(r'\\d+','',data)\r\n",
        "  data = [i for i in data.split() if not any(x in i for x in ['@','http'])]\r\n",
        "  data = [i for i in data if not len(i)<2]\r\n",
        "  data = [lemmatizer.lemmatize(i) for i in data]\r\n",
        "  data = [any2utf8(i) for i in data]\r\n",
        "  return data\r\n",
        "\r\n",
        "def original_scorer(worda_count, wordb_count, bigram_count, len_vocab, min_count, corpus_word_count):\r\n",
        "  return (bigram_count - min_count)/worda_count/wordb_count*len_vocab\r\n",
        "\r\n",
        "def score_candidate(word_a, word_b, in_between, phraser_dict):\r\n",
        "  word_a_cnt = phraser_dict[\"phraser_model\"].vocab[word_a]\r\n",
        "  if word_a_cnt <= 0:\r\n",
        "    return None, None\r\n",
        "\r\n",
        "  word_b_cnt = phraser_dict[\"phraser_model\"].vocab[word_b]\r\n",
        "  if word_b_cnt <= 0:\r\n",
        "    return None, None\r\n",
        "  \r\n",
        "  phrase = b'_'.join([word_a] + in_between + [word_b])\r\n",
        "  phrase_cnt = phraser_dict[\"phraser_model\"].vocab[phrase]\r\n",
        "\r\n",
        "  if phrase_cnt <=0:\r\n",
        "    return None, None\r\n",
        "\r\n",
        "  score = original_scorer(worda_count = word_a_cnt, wordb_count = word_b_cnt, bigram_count = phrase_cnt,\r\n",
        "                          len_vocab = phraser_dict[\"vocab_length\"],\r\n",
        "                          min_count = phraser_dict[\"phraser_model\"].min_count,\r\n",
        "                          corpus_word_count = phraser_dict[\"phraser_model\"].corpus_word_count)\r\n",
        "  \r\n",
        "  if score <= phraser_dict[\"phraser_model\"].threshold:\r\n",
        "    return None, None\r\n",
        "\r\n",
        "  return phrase, score\r\n",
        "\r\n",
        "\r\n",
        "def analyze_sentence(sentence, phraser_dict):\r\n",
        "  sentence = prep(sentence)\r\n",
        "  start_token, in_between = None, []\r\n",
        "  for word in sentence:\r\n",
        "    if word not in common_terms:\r\n",
        "      if start_token:\r\n",
        "        phrase, score = score_candidate(start_token, word, in_between, phraser_dict)\r\n",
        "        if score is not None:\r\n",
        "          yield phrase\r\n",
        "          start_token, in_between = None, []\r\n",
        "        else:\r\n",
        "          yield start_token\r\n",
        "          for w in in_between:\r\n",
        "            yield w\r\n",
        "          start_token, in_between = word, []\r\n",
        "      else:\r\n",
        "        start_token, in_between = word, []\r\n",
        "    else:\r\n",
        "      if start_token:\r\n",
        "        in_between.append(word)\r\n",
        "      else:\r\n",
        "        yield word\r\n",
        "  if start_token:\r\n",
        "    yield start_token\r\n",
        "    for w in in_between:\r\n",
        "      yield w\r\n",
        "\r\n",
        "# Compute Sentence Vector for a given input sentence\r\n",
        "def compute_sentence_vector(sentence):\r\n",
        "  sentence = [any2unicode(word) for word in analyze_sentence(sentence, phraser_dict)]\r\n",
        "  if len(sentence) == 0:\r\n",
        "    return None\r\n",
        "  word_indices = [optimized_model_s2v_usif[\"word_index\"][word] for word in sentence if word in optimized_model_s2v_usif[\"word_index\"]]\r\n",
        "  weighted_vector = np_sum(np_mult(optimized_model_s2v_usif[\"word_vectors\"][word_indices],\r\n",
        "                                   optimized_model_s2v_usif[\"word_weights\"][word_indices][:,None]), axis=0)\r\n",
        "  weighted_vector *= 1/len(word_indices)\r\n",
        "  sentence_vector = weighted_vector - weighted_vector.dot(w_comp.transpose()).dot(w_comp)\r\n",
        "  return sentence_vector"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHaMX_GlELID"
      },
      "source": [
        "# Loading the optimized version of Phraser and Sentence2Vec models\r\n",
        "phraser_dict = pickle.load(open('/content/drive/MyDrive/Application/Case Studies/W2V S2V Optimization/optimized_model_bigram_phraser.pkl', 'rb'))\r\n",
        "optimized_model_s2v_usif = pickle.load(open('/content/drive/MyDrive/Application/Case Studies/W2V S2V Optimization/optimized_model_s2v_usif.pkl', 'rb'))\r\n",
        "\r\n",
        "svd_weights = (optimized_model_s2v_usif['svd_residuals'][0]**2)/(optimized_model_s2v_usif['svd_residuals'][0]**2).sum().astype(REAL)\r\n",
        "w_comp = optimized_model_s2v_usif['svd_residuals'][1]*(svd_weights[:,None].astype(REAL))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hf7X9eiIomm",
        "outputId": "d86eb7b8-1e41-46c4-b857-e267194b8791"
      },
      "source": [
        "# Testing on examples\r\n",
        "sentence1 = \"room was very small\"\r\n",
        "sentence2 = \"Room was a bit small\"\r\n",
        "\r\n",
        "vector1 = compute_sentence_vector(sentence1)\r\n",
        "vector2 = compute_sentence_vector(sentence2)\r\n",
        "\r\n",
        "cosine_similarity([vector1], [vector2])[0][0]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.65712184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}